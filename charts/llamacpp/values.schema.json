{
   "$schema": "http://json-schema.org/draft-07/schema#",
   "type": "object",
   "properties": {
      "modelNameOverride": {
         "description": "Model name to use (defaults to model_id) during inference",
         "type": [
            "string",
            "null"
         ]
      },
      "workingMemory": {
         "description": "Temporary storage to use to cache model weights (in GB), should be big enough to hold the model weights. All workers should have enough free disk to accommodate the weights.",
         "type": "integer"
      },
      "workers": {
         "description": "Number of workers, corresponding to the number of nodes (or machines) that will be used to deploy the model",
         "type": "integer"
      },
      "backend": {
         "description": "Backend to run workers. One of the following: cpu, cuda, rocm",
         "type": "string"
      },
      "repoId": {
         "description": "Huggingface model id to deploy",
         "type": "string"
      },
      "modelFilename": {
         "description": "Specific GGUF model file to use, which let's you select the quantization level",
         "type": "string"
      },
      "hfToken": {
         "description": "Huggingface access token, only required to load gated model weights",
         "type": "string"
      },
      "cpus": {
         "description": "CPUs to be used per single worker (final one = cpus * workers). Workers should have these many CPUs available.",
         "type": "integer"
      },
      "parallel": {
         "description": "Number of sequences to decode in parallel.",
         "type": "integer"
      },
      "memory": {
         "description": "RAM memory to be used per single worker (final one = memory * workers). Workers should have these much RAM available.",
         "type": "integer"
      },
      "cudaGpuMemPercentage": {
         "description": "Maximum memory fraction allowed to be used from the GPU vRAM.",
         "type": "integer"
      },
      "serverExtra": {
         "description": "Extra parameters for deployment. See https://github.com/ggerganov/llama.cpp/tree/master/examples/server",
         "type": [
            "string",
            "null"
         ]
      },
      "nodeport": {
         "description": "Specify the node port for the service that exposes the job",
         "type": [
            "integer",
            "null"
         ]
      },
      "litellmBaseUrl": {
         "description": "Base URL of the LiteLLM service (central model registry). This can be an external LiteLLM instance or an internal deployment in the pool (default)",
         "type": [ 
            "string",
            "null"
         ]
      },
      "litellmKey": {
         "description": "Master key of the LiteLLM service (central model registry)",
         "type": [
            "string",
            "null"
         ]
      },
      "litellmAccessGroup": {
         "description": "What access group to assign the model to (ringfencing)",
         "type": [
            "string",
            "null"
         ]
      },
      "litellmKalavaiExtras": {
         "description": "Extra information to store in the LiteLLM model registration (must be a dictionary)",
         "type": "string"
      },
      "lagoUrl": {
         "description": "Base URL of the Lago service (payment system)",
         "type": [
            "string",
            "null"
         ]
      },
      "lagoApiKey": {
         "description": "Master key of the Lago service",
         "type": [
            "string",
            "null"
         ]
      },
      "lagoExternalSubscriptionId": {
         "description": "ID of the Lago subscription to send events to",
         "type": [
            "string",
            "null"
         ]
      },
      "lagoEventCode": {
         "description": "Event code to use when submitting event to Lago payment API",
         "type": [
            "string",
            "null"
         ]
      },
      "lagoEventInterval": {
         "description": "Interval in seconds to send the lago event",
         "type": [
            "string",
            "null"
         ]
      },
      "system": {
         "properties": {
            "priorityClassName": {
               "type": "string",
               "description": "PriorityClassName to use for the deployment"
            },
            "nodeSelectors": {
               "type": [
                  "object",
                  "null"
               ],
               "description": "target specific devices. Expected format -> key: value pairs, where 'value' is a list of elements"
            },
            "nodeSelectorsOps": {
               "type": "string",
               "enum": [
                  "OR",
                  "AND"
               ],
               "description": "Logical operation to use to apply to nodeSelectors. Accepted values: 'OR', 'AND'"
            },
            "jobId": {
               "type": "string",
               "description": "Id to tag spawned objects."
            }
         }
      }
   },
   "required": [
      "workingMemory",
      "workers",
      "backend",
      "repoId",
      "modelFilename",
      "hfToken"
   ]
}